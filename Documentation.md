# Introduction
This project leverages natural language processing (NLP) techniques, Large Language Models (LLMs) and embeddings to create an intelligent question-answering system. 

Comprising two main scripts, `preprocess.py` and `app.py`, the project automates the collection of textual data from a specified website, processes this data to generate meaningful numerical representations (embeddings), and utilises these embeddings to provide contextually relevant answers to user queries.

## `preprocess.py` - Data Collection and Preparation
`preprocess.py` crawls web pages within a specified domain and systematically navigates through the website, extracting text from each page it encounters. The collected text undergoes initial preprocessing to clean and organise the data, making it suitable for further analysis.

The script then employs OpenAI's API to generate embeddings for each piece of text. These embeddings capture the semantic essence of the text in a high-dimensional space, facilitating the identification of contextual similarities between different texts. The processed data, along with its embeddings, is saved for subsequent use, laying the groundwork for the question-answering capabilities of the system.

## `app.py` - Flask Application for Question Answering
With the data prepared, `app.py` serves as the interface between the user and the system's NLP engine. This script initiates a Flask web application, providing endpoints for users to submit their questions.

Upon receiving a query, the application leverages the previously generated embeddings to find the most relevant context within the collected data. It then formulates this context and the user's question as input for an OpenAI GPT model. 

The model, trained on vast amounts of text from the internet, generates an answer that reflects both the specific information contained in the crawled data and its understanding of the topic at large. The answer is then returned to the user through the web interface, completing the cycle of query and response.

## Integration and Workflow
The integration of `preprocess.py` and `app.py` creates a workflow that bridges web crawling and NLP-driven question answering. Initially, `preprocess.py` lays the foundation by collecting and preparing the data, which `app.py` subsequently utilises to offer real-time answers. This allows the system to provide answers that are not only contextually relevant but also deeply informed by the specific context of the targeted domain. Users interact with the system through a straightforward web interface, making complex NLP capabilities accessible to anyone with a question to ask.

## Use-cases
Together, these scripts leverage sophisticated machine learning capabilties to demonstrate  how existing data from websites can be harnessed to build powerful and interactive AI-driven ways to retrieve and discovery knowledge.

For example, the basic capabilities demonstrated in this project could be applied to create a contextually-aware chatbot on a website. 

# Overview
There are two main scripts: `preprocess.py` and `app.py`. 
- `preprocess.py` crawls a specified domain to collect text data, preprocess and generate embeddings for the collected data
- `app.py` provides answers to questions based on the generated embeddings using OpenAI's GPT model. The question and answers are received and served via a simple Flask application.

## preprocess.py

### Main Functionalities
- **Crawling**: The script starts by defining a domain to crawl and collects hyperlinks within that domain to gather text data.
- **HTML Parsing**: A custom HTMLParser class, HyperlinkParser, is defined to extract `href` attributes from `<a>` tags.
- **Text Extraction and Cleaning**: Text is extracted from crawled pages and cleaned to remove unnecessary whitespace and newline characters.
- **Embedding Generation**: Using OpenAI's API, embeddings for the cleaned text are generated and stored.

### Functions
- `get_hyperlinks(url)`: Extracts hyperlinks from the specified URL.
- `get_domain_hyperlinks(local_domain, url)`: Filters hyperlinks to include only those within the specified domain.
- `crawl(url)`: Orchestrates the crawling process, collecting text data and saving it.
- `remove_newlines(serie)`: Cleans text data by removing newline characters.
- `generate_embeddings(df)`: Generates embeddings for the text data and saves the results.

## app.py

### Flask Application
- Initialises a Flask app and loads a DataFrame of embeddings generated by `preprocess.py`.
- Defines routes for answering questions and serving static files.

### Main Functionalities
- **Context Creation**: Given a question, the script creates a context by finding the most similar texts based on embeddings.
- **Answer Generation**: Utilises OpenAI's GPT model to generate answers based on the context created for the given question.

### Functions
- `create_context(question, df, max_len, size)`: Creates context for a question from the DataFrame based on embeddings similarity.
- `answer_question(df, question, model, max_len, size, debug, max_tokens, stop_sequence)`: Generates an answer to the given question using the context created by create_context.

**Flask Routes:**
- `ask()`: Accepts POST requests with a question and returns the generated answer.
- `home()`: Serves the static homepage.

# Running the Application
Ensure all dependencies are installed and environment variables are set.
Run `preprocess.py` to crawl the domain and generate embeddings.
Start the Flask application by running `app.py`.

# Detailed overview

## preprocess.py

### Define a class for HTML Parsing

This code defines a class that will parse HTML and extract hyperlinks.

```python
class HyperlinkParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.hyperlinks = [] # Initialises an empty list to store URLs of hyperlinks

    def handle_starttag(self, tag, attrs): 
        attrs = dict(attrs)
        if tag == "a" and "href" in attrs:
            self.hyperlinks.append(attrs["href"])
```

The `def handle_starttag()` method gets called whenever the parser encounters the start of a tag in the HTML content. It receives two arguments:
- `tag` - Is the name of the tag the parser has encountered.
- `attrs` -  A list of `(attribute, value)` pairs containing the attributes found inside the tag's opening bracket.

The method converts `attrs` into a dictionary for easier access. If the current tag is an `<a>` tag (which denotes a hyperlink) and it has an `href` attribute (the attribute that specifies the URL the link points to), the URL (value of the `href` attribute) is appended to the `self.hyperlinks` list. This effectively collects all URLs pointed to by hyperlinks in the parsed HTML content.

#### How it works
When an instance of `HyperlinkParser` is used to parse HTML content, the parser systematically goes through the content and triggers the `handle_starttag` method for each starting tag it encounters. For every `<a>` tag with an `href` attribute, the URL specified in the `href` is stored in the `self.hyperlinks` list. After the parsing is complete, `self.hyperlinks` will contain all the extracted URLs, making it possible to process or analyze the hyperlinks further, such as filtering them based on certain criteria or using them to crawl linked pages.

### Extract and return all hyperlinks.

```python
def get_hyperlinks(url):

    # Try to open the URL and read the HTML
    try:
        # Open the URL and read the HTML
        with urllib.request.urlopen(url) as response:

            # If the response is not HTML, return an empty list
            if not response.info().get('Content-Type').startswith("text/html"):
                return []

            # Decode the HTML
            html = response.read().decode('utf-8')
    except Exception as e:
        print(e)
        return []

    # Create the HTML Parser and then Parse the HTML to get hyperlinks
    parser = HyperlinkParser()
    parser.feed(html)

    return parser.hyperlinks
```

The `get_hyperlinks(url)` function will extract and return all hyperlinks from a webpage specified by its URL. It will:

**1. Open the URL:** The function starts by trying to open the specified URL using the `urlopen` method from the `urllib.request module`. This method attempts to access the webpage and returns a response object if successful.

**2. Handle Non-HTML Responses:** Once the URL is opened, the function checks the `Content-Type` of the response to ensure it is an HTML document. 

> This is important because the function is designed to parse HTML content, and other types of content (like images or PDF files) would not contain hyperlinks in the format expected. If the content type does not start with "**text/html**", indicating it's not an HTML document, the function returns an empty list, as there are no hyperlinks to extract.

**3. Read and Decode the HTML:** If the content type is HTML, the function reads the response body using the `read()` method, which returns the HTML content as a byte string. This byte string is then decoded to a regular string (assuming UTF-8 encoding), representing the HTML source code of the webpage.

**4. Parse the HTML for Hyperlinks:** With the HTML content now in a string format, the function creates an instance of the `HyperlinkParser` class defined earlier. This custom parser is an `HTMLParser` that specifically looks for `<a>` tags and extracts the URLs specified in their href attributes. The `feed()` method of the parser is called with the HTML content, which triggers the parsing process. As the parser encounters `<a>` tags, it collects the URLs of the hyperlinks they contain and stores them in its `hyperlinks` list attribute.

**5. Return the Extracted Hyperlinks:** After the HTML content has been fully parsed, the function returns the list of hyperlinks collected by the `HyperlinkParser` instance. This list contains the URLs of all hyperlinks found on the webpage, which can then be used for further processing.

**Error handling:** The function includes a `try-except` block to handle any exceptions that might occur during the process of opening the URL, reading the response, or parsing the HTML. Common issues could include network errors, invalid URLs, or server-side errors causing a non-200 response. If any exception is caught, the function prints the exception message and returns an empty list, indicating that no hyperlinks could be extracted due to an error.

### Fetch and filter hyperlinks.

The `get_domain_hyperlinks(local_domain, url)` function fetches the hyperlinks and filters those links so that only the ones belonging to the specified `local_domain` are returned.

```python
# Function to get the hyperlinks from a URL that are within the same domain
def get_domain_hyperlinks(local_domain, url):
    clean_links = []
    for link in set(get_hyperlinks(url)):
        clean_link = None

        # If the link is a URL, check if it is within the same domain
        if re.search(HTTP_URL_PATTERN, link):
            # Parse the URL and check if the domain is the same
            url_obj = urlparse(link)
            if url_obj.netloc == local_domain:
                clean_link = link

        # If the link is not a URL, check if it is a relative link
        else:
            if link.startswith("/"):
                link = link[1:]
            elif link.startswith("#") or link.startswith("mailto:"):
                continue
            clean_link = "https://" + local_domain + "/" + link

        if clean_link is not None:
            if clean_link.endswith("/"):
                clean_link = clean_link[:-1]
            clean_links.append(clean_link)

    # Return the list of hyperlinks that are within the same domain
    return list(set(clean_links))
```

Here is a breakdown:

**1. Fetch All Hyperlinks:** The function starts by calling `get_hyperlinks(url)`, which retrieves all hyperlinks from the given URL's webpage. This returns a list of URLs found in `<a>` tags on the page.

**2. Initialise a List for Clean Links:** A list named `clean_links` is initialised to store the filtered hyperlinks that are within the specified domain.

**3. Iterate Through the Links:** The function iterates through the set of links returned by `get_hyperlinks(url)` to ensure each link is processed only once, even if it appears multiple times on the page.

**URL Validation and Domain Check:**
- For each link, the function first checks if it is an absolute URL that includes the domain name. This is done using the `re.search(HTTP_URL_PATTERN, link)` which matches links that start with "http://" or "https://".
- If the link is an absolute URL, the function uses `urlparse(link)` to parse the URL and compare its domain (`netloc`) with the `local_domain` specified. If they match, the link is considered "clean" and belongs to the same domain.
- If the link does not match the `HTTP_URL_PATTERN`, it's considered a relative link or a special link (like those starting with `#` for page anchors or `mailto:` for email addresses). Relative links are presumed to be internal to the domain, but special links are ignored.

**5. Handling Relative Links:** For relative links, the function constructs a full URL by concatenating "https://" with the `local_domain` and the relative path. It ensures that links starting with "/" don't duplicate the slash when concatenated with the domain.

**6. Final URL Adjustments:** Before adding a URL to `clean_links`, the function checks if it ends with a slash ("/") and removes it if present. This normalisation step ensures consistency in the URLs being processed and stored.

**7. Return Unique Clean Links:** Finally, the function returns a list of unique clean links that are within the same domain. This is achieved by converting `clean_links` to a set and then back to a list, which removes any duplicates.

This function is particularly useful in web crawling tasks where the goal is to systematically explore and collect data from a website by following links that keep the crawler within the same domain, thereby avoiding external sites.

### Extract content from links

The `crawl(url)` function is deisgned to systematically visit the links, extract, and save the text content of each page to a file. It also follows the links to other pages within the same domain to continue the crawling process.

```python
def crawl(url):
    # Parse the URL and get the domain
    local_domain = urlparse(url).netloc

    # Create a queue to store the URLs to crawl
    queue = deque([url])

    # Create a set to store the URLs that have already been seen (no duplicates)
    seen = set([url])

    # Create a directory to store the text files
    if not os.path.exists("text/"):
            os.mkdir("text/")

    if not os.path.exists("text/"+local_domain+"/"):
            os.mkdir("text/" + local_domain + "/")

    # Create a directory to store the csv files
    if not os.path.exists("data"):
            os.mkdir("data")

    # While the queue is not empty, continue crawling
    while queue:

        # Get the next URL from the queue
        url = queue.pop()
        print(url) # for debugging and to see the progress

        # Save text from the url to a <url>.txt file
        with open('text/'+local_domain+'/'+url[8:].replace("/", "_") + ".txt", "w", encoding="UTF-8") as f:

            # Get the text from the URL using BeautifulSoup
            soup = BeautifulSoup(requests.get(url).text, "html.parser")

            # Get the text but remove the tags
            text = soup.get_text()

            # If the crawler gets to a page that requires JavaScript, it will stop the crawl
            if ("You need to enable JavaScript to run this app." in text):
                print("Unable to parse page " + url + " due to JavaScript being required")

            # Otherwise, write the text to the file in the text directory
            f.write(text)

        # Get the hyperlinks from the URL and add them to the queue
        for link in get_domain_hyperlinks(local_domain, url):
            if link not in seen:
                queue.append(link)
                seen.add(link)

crawl(full_url)
```

Here is a breakdown:

**1. Parse the Starting URL:** The function uses `urlparse(url).netloc` to extract the domain name (`local_domain`) from the starting URL. This domain name is used to limit the crawl to pages within the same domain.

**2. Initialise Data Structures:**
- A queue `queue` (implemented with `deque`) is initialised with the starting URL. The queue manages the URLs to visit in a first-in, first-out (FIFO) manner.
- A set `seen` is initialised with the starting URL to keep track of URLs that have already been visited or seen, preventing the crawler from visiting the same page multiple times.

**3. Prepare Directories:** The function checks if the directories for storing text files (`text/<domain>/`) and CSV files (`data`) exist. If not, it creates them. This ensures that there's a place to save the extracted data.

**Crawling Loop:**
- The crawler enters a loop that continues as long as there are URLs in the `queue` to be visited.
- It takes the next URL from the `queue`, prints it for debugging purposes, and proceeds to process it.

**5. Extract and Save Text Content:**
- The function fetches the HTML content of the current URL using `requests.get(url).text` and parses it with `BeautifulSoup` to create a `soup` object.
- It then extracts the textual content of the page using `soup.get_text()`, which removes all HTML tags, leaving only the text.
- The text is saved to a file named after the URL (with slashes replaced by underscores to form a valid filename) in the `text/<domain>/` directory. Before writing, it checks if the text indicates that JavaScript is required to view the page content, in which case it prints a message and skips saving.

**6. Follow Hyperlinks:**
- The function calls `get_domain_hyperlinks(local_domain, url)` to get a list of hyperlinks from the current page that are within the same domain.
- It adds these hyperlinks to the `queue` if they haven't been seen before, updating the seen set accordingly. This way, the crawler systematically explores the website, following links to discover and process all accessible pages within the domain.

**7. Recursive Crawling:** By adding new links to the `queue` and marking visited URLs in `seen`, the crawler recursively visits the entire website or until no more new links are found within the domain.

This crawler is effective for extracting text data from websites for analysis, search indexing, or archiving purposes. It respects the site's domain boundaries, ensuring that it does not stray into external sites.

### Remove new lines

The `remove_newlines(serie)` function cleans up the text by removing newline characters  (`\n` and `\\n`) and replacing multiple consecutive spaces with a single space. It's applied to a Pandas Series (`serie`), which  contains the text data extracted from web pages.

```python
def remove_newlines(serie):
    print("Tidying up files, please wait...") 
    serie = serie.str.replace('\n', ' ')
    serie = serie.str.replace('\\n', ' ')
    serie = serie.str.replace('  ', ' ')
    serie = serie.str.replace('  ', ' ')
    return serie
# Blank empty lines can clutter the text files and make them harder to process. A simple function can remove those lines and tidy up the files.
```
### Read text files

The script iterates through all text files in `text/<domain>` and reads the content of each file. It performs preliminary cleaning on the file names and associates them with their content.
```python

# Create a list to store the text files
texts=[]

# Get all the text files in the text directory
for file in os.listdir("text/" + domain + "/"):

    # Open the file and read the text
    with open("text/" + domain + "/" + file, "r", encoding="UTF-8") as f:
        text = f.read()

        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.
        texts.append((file[11:-4].replace('-',' ').replace('_', ' ').replace('#update',''), text))
```

### Create DataFrames

This section creates a dataframe, cleans the text further and saves the final output to a CSV file.

```python

# Create a dataframe from the list of texts
df = pd.DataFrame(texts, columns = ['fname', 'text'])

# Set the text column to be the raw text with the newlines removed
df['text'] = df.fname + ". " + remove_newlines(df.text)
df.to_csv('data/scraped.csv')
df.head() ## whats this do
```

### Tokenise and Token Count Analysis

This section tokenises the data to prepare it for embedding generation and other NLP tasks.

```python

import tiktoken
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load the cl100k_base tokenizer which is designed to work with the ada-002 model
tokenizer = tiktoken.get_encoding("cl100k_base")

df = pd.read_csv('data/scraped.csv', index_col=0)
df.columns = ['title', 'text']

# Tokenize the text and save the number of tokens to a new column
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))

# Visualize the distribution of the number of tokens per row using a histogram
df.n_tokens.hist()

max_tokens = 500
```

`tiktoken.get_encoding("cl100k_base")` is used to load a tokenizer that is compatible with the `ada-002` model. This tokenizer is applied to the text data to tokenize it.

 For each row in the DataFrame, the number of tokens (after tokenization) is calculated and stored in a new column (`n_tokens`). This allows for analysing the length of documents in terms of tokens, which is relevant for understanding the dataset's distribution and for optimizing model input sizes.

Histogram Visualization: A histogram of the `n_tokens` column is generated to visualize the distribution of token counts across the dataset. This can help identify common document lengths and any outliers, informing decisions about data preprocessing (e.g., splitting long documents) and model configuration (e.g., setting appropriate maximum token limits).

The `max_tokens` variable is set to 500, indicating a threshold or parameter that could be used in subsequent processing steps, such as when generating embeddings or splitting documents into smaller chunks to ensure they fit within the input size limits of certain models.

This section of the code is crucial for preparing scraped web content for NLP tasks, ensuring data cleanliness, and structuring the data in a way that facilitates easy analysis and processing with machine learning models.

### Create smaller chunks

The `split_into_many()` function splits blocks of text into smaller chunks, with each containing no more than the specified number of tokens (`max_tokens`).

```python
# Function to split the text into chunks of a maximum number of tokens
def split_into_many(text, max_tokens = max_tokens):

    # Split the text into sentences
    sentences = text.split('. ')

    # Get the number of tokens for each sentence
    n_tokens = [len(tokenizer.encode(" " + sentence)) for sentence in sentences]

    chunks = []
    tokens_so_far = 0
    chunk = []

    # Loop through the sentences and tokens joined together in a tuple
    for sentence, token in zip(sentences, n_tokens):

        # If the number of tokens so far plus the number of tokens in the current sentence is greater
        # than the max number of tokens, then add the chunk to the list of chunks and reset
        # the chunk and tokens so far
        if tokens_so_far + token > max_tokens:
            chunks.append(". ".join(chunk) + ".")
            chunk = []
            tokens_so_far = 0

        # If the number of tokens in the current sentence is greater than the max number of
        # tokens, go to the next sentence
        if token > max_tokens:
            continue

        # Otherwise, add the sentence to the chunk and add the number of tokens to the total
        chunk.append(sentence)
        tokens_so_far += token + 1

    return chunks
```

This is particularly useful for preparing text for processing with models that have a maximum input size limit. Here's a breakdown of how it works:

**Parameters**
- `text` is the block of text to be split
- `max_tokens` is an optional paramater to specify the maximum number of tokens allowed per chunk. It defaults to a global `max_tokens` value defined elsewhere.

**Logic**
**1. Sentence Splitting:** The function starts by splitting the `text` into sentences based on the period followed by a space (`.`) delimiter. This approach assumes that sentences are a good boundary for splitting, aiming to preserve sentence integrity within each chunk.

**2. Token Count Calculation:**
- For each sentence, the function calculates the number of tokens it contains by encoding the sentence using the previously loaded `tokenizer` (from `tiktoken.get_encoding("cl100k_base")`).
- The `tokenizer.encode` method is called with a space concatenated before the sentence to ensure proper tokenization, especially at the beginning of sentences.
- The result is a list, `n_tokens`, where each element corresponds to the token count of each sentence.

**3. Chunk Assembly:**
- The function then iterates through each sentence and its associated token count, attempting to assemble chunks of text that adhere to the `max_tokens` limit.
- `chunks` is a list to store the final text chunks.
- `tokens_so_far` tracks the cumulative number of tokens in the current chunk being assembled.
- `chunk` is a temporary list to hold sentences that will form the current chunk.

**4. Finalise chunks**
- In case there's remaining text in the `chunk` after the loop (meaning the text didn't reach another `max_tokens` limit), this text is concatenated and added to `chunks` as the final chunk. This ensures completeness of the text processing, with no content left behind

**Process**
As it iterates through sentences, the function checks if adding the next sentence would exceed the max_tokens limit. If it would:
- The sentences accumulated in `chunk` so far are joined with `.` (to reconstruct the original sentence boundaries) and added as a new entry in `chunks`.
- `chunk` and `tokens_so_far` are reset to start assembling a new chunk.

If a sentence's token count alone exceeds `max_tokens`, that sentence is skipped. 

> *This is a simplification and might not be ideal for all use cases, as it could result in the omission of important text. Depending on the application, you might want to further split such sentences or handle them differently.*

This process continues until all sentences have been processed. If there's any remaining text in `chunk` after the loop, it's also joined and added to `chunks`.

The function ultimately returns `chunks`, a list of text segments where each segment is designed to be within the specified `max_tokens` limit, making them suitable for processing with token limit-constrained models or APIs.


### Check DataFrame does not exceed `max_tokens`.

This segment is designed to process a DataFrame containing text data by ensuring each piece of text does not exceed a specified maximum number of tokens (`max_tokens`). It aims to create a new DataFrame where each row contains text within the token limit, using the previously defined `split_into_many` function to split longer texts into appropriately sized chunks.

```python
shortened = []

# Loop through the dataframe
for row in df.iterrows():

    # If the text is None, go to the next row
    if row[1]['text'] is None:
        continue

    # If the number of tokens is greater than the max number of tokens, split the text into chunks
    if row[1]['n_tokens'] > max_tokens:
        shortened += split_into_many(row[1]['text'])

    # Otherwise, add the text to the list of shortened texts
    else:
        shortened.append( row[1]['text'] )

df = pd.DataFrame(shortened, columns = ['text'])
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))
df.n_tokens.hist()
```
Here's a breakdown of its functionality:
**- `shortened` List:** Initialises an empty list named `shortened` to store the processed texts, which will either be the original texts (if they're within the token limit) or chunks of texts (if the original text exceeds the token limit).

The code iterates over each row in the DataFrame `df` using `df.iterrows()`, which provides an iterator yielding index and row data for each row.

**1. Skip Null Texts:** If the text column of the current row is `None` (indicating missing or null text), the loop continues to the next iteration without adding anything to `shortened`.

**2. Handling Long Texts:** If the `n_tokens` value for the row's text exceeds `max_tokens`, indicating that the text is too long based on the token limit:
- The `split_into_many` function is called with the row's text, resulting in a list of text chunks, each adhering to the token limit. These chunks are then added (`+=`) to the `shortened` list.

**3. Handling Short Texts:** If the text's token count does not exceed `max_tokens`, the original text is directly appended to the `shortened` list as it already complies with the token limit.

**Creating a New DataFrame**
After processing all rows, a new DataFrame is created from the `shortened` list, with each element (text or chunk of text) becoming a row in the new DataFrame. This ensures that all text entries in the new DataFrame are within the specified token limit.

**Recalculating Token Counts**
The new DataFrame recalculates the number of tokens for each text (or text chunk) using the tokenizer's `encode` method. This calculation is stored in a new `n_tokens` column, providing an updated token count for each row based on the potentially modified texts.

**Visualizing Token Distribution**
Finally, the code generates a histogram of the `n_tokens` column in the new DataFrame. This visualization helps understand the distribution of token counts across the processed texts, ensuring that the splitting logic has effectively regulated text lengths according to the `max_tokens` limit.

### Generate embeddings

The `generate_embeddings(df)` function is designed to generate embeddings for each piece of text contained within a DataFrame `df` using OpenAI's API, specifically leveraging the `text-embedding-ada-002` model. This function enriches the DataFrame by adding a new column (`embeddings`) that holds the numerical vector representation (embedding) of the text, which is crucial for many natural language processing (NLP) and machine learning tasks. 

```python
def generate_embeddings(df):
    # Assuming tiktoken and the use of OpenAI's API are setup correctly
    df['embeddings'] = df['text'].apply(lambda x: client.embeddings.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])
    df.to_csv('data/embeddings.csv')
    df.head()
    print("Embeddings generated and saved to 'data/embeddings.csv'.")
```

Here's how the function operatings

**Process Overview**
**1. Applying the Embeddings Creation:** The function iterates over each row's `text` column in the DataFrame `df` by using the `.apply()` method. For each piece of text (`x`), it calls `client.embeddings.create(input=x, engine='text-embedding-ada-002')`, which sends a request to OpenAI's API to generate an embedding for the text. The `engine` parameter specifies the model used for generating embeddings, in this case, `text-embedding-ada-002`, which is tailored for creating text embeddings.

**2. Extracting the Embedding:** After the API call, the function extracts the actual embedding from the API's response. OpenAI's API response for embeddings includes a data field containing a list of result objects, where each object holds an `embedding` among other metadata. The function navigates through this structure (`['data'][0]['embedding']`) to access the first (and typically only) embedding in the response.

**3. Storing Embeddings:** The extracted embeddings are stored in a new column in the DataFrame, `df['embeddings']`. This operation effectively adds a high-dimensional vector representation for each text to the DataFrame, enabling advanced NLP tasks that rely on these numerical representations.

**4. Saving the Enhanced DataFrame:** Once all rows have been processed and their embeddings generated and stored, the function saves the updated DataFrame to a CSV file, `data/embeddings.csv`. This step persists the data with embeddings to disk, making it reusable for future analysis or modeling without needing to regenerate embeddings.

**5. Preview and Confirmation:** The function prints the first few rows of the updated DataFrame using `df.head()`, providing a quick preview of the embeddings. Additionally, it prints a confirmation message indicating that the embeddings have been generated and saved successfully.

**Use Cases and Significance**
Embeddings are foundational to many modern NLP applications. They provide a way to represent text as dense vectors that capture semantic meaning and relationships between words or texts. Applications include but are not limited to:

**- Semantic Similarity:** Determining how similar two pieces of text are by comparing their embeddings.
**- Text Classification:** Using embeddings as features for machine learning models to categorize texts.
**- Information Retrieval:** Enhancing search algorithms with embeddings to find relevant documents based on semantic similarity to a query.

By generating and storing embeddings for text data, `generate_embeddings(df)` prepares the dataset for these advanced NLP tasks, enabling deeper analysis and more intelligent applications based on the semantic content of the texts.

### Entry point

This section of the code represents the entry point for executing the script when it's run as a standalone program. The `if __name__ == "__main__":` check is a common Python idiom used to determine if the script is being run directly by the interpreter as opposed to being imported as a module in another script.

```python
################################################################################
### Main Functionality
################################################################################

if __name__ == "__main__":
    crawl(full_url)
    generate_embeddings(df)
    print("Preprocessing complete. Embeddings are ready.")
```

Here's a breakdown of what happens within this block:

**Crawling Web Pages:** The script begins its execution by calling the `crawl(full_url)` function. This function is tasked with crawling web pages starting from the `full_url`, which was defined earlier in the script as the entry point URL for the web crawling process. The crawl function navigates through the website located at `full_url`, collecting text from the pages it visits and saving this text to files organized by domain. This process also involves creating a queue of URLs to visit, ensuring only new and relevant (same domain) URLs are followed, thereby systematically exploring the content of the website.

**Generating Embeddings:** After crawling and collecting text data, the script calls `generate_embeddings(df)`, which assumes that a DataFrame `df` has been prepared with the text data extracted during the crawling process. This function processes each piece of text in the DataFrame, generating numerical vector representations (embeddings) for each using OpenAI's API. These embeddings are then added to the DataFrame, enriching it with data that captures the semantic essence of the texts in a format that's conducive to various NLP tasks. The enhanced DataFrame is saved to a CSV file, making the embeddings accessible for future use without the need to recompute them.

**Completion Message:** Finally, the script prints a message, "Preprocessing complete. Embeddings are ready.", signaling the end of the preprocessing workflow. This message indicates that the script has successfully completed its main tasks of web crawling, text data collection, and embeddings generation, and that the data is now ready for further analysis or modeling.

**Significance**
This entry point encapsulates a complete workflow for data collection and preprocessing, specifically tailored for NLP applications. By automating the process of web crawling, text extraction, and embeddings generation, the script provides a streamlined pathway to prepare raw web content for advanced NLP tasks, such as semantic analysis, machine learning modeling, or information retrieval systems. 

The automated generation of embeddings, in particular, transforms the collected text into a form that's directly usable in data-driven NLP models, significantly reducing the manual effort required for data preparation and enabling more sophisticated analyses.

## app.py

### Flask app initialisation

This line initializes a new Flask application instance. Flask is a Python web framework that allows for the development of web applications.

```python
# Initialize Flask app
app = Flask(__name__, static_folder='static')
```

- ` __name__` is a special variable in Python that is used here to determine the root path of the Flask application. This enables Flask to know where to look for resources like templates and static files.
- `static_folder='static'` specifies the directory within the application that Flask will use to serve static files (e.g., JavaScript, CSS, images). By default, Flask serves static files from a folder named `static` in the root path of the application. This parameter explicitly sets the folder name, enhancing readability and maintainability.

### Loading the Embeddings DataFrame

This part loads a CSV file containing pre-generated embeddings into a Pandas DataFrame `df`. The embeddings are stored in `data/embeddings.csv`.

```python
# Load the embeddings DataFrame
df = pd.read_csv('data/embeddings.csv', index_col=0)
df['embeddings'] = df['embeddings'].apply(literal_eval).apply(np.array)
```

- `pd.read_csv('data/embeddings.csv', index_col=0)` uses Pandas to read the CSV file. The `index_col=0` argument tells Pandas to use the first column of the CSV file as the DataFrame's index.

- The embeddings in the CSV are likely stored as strings representing lists or arrays. `df['embeddings'].apply(literal_eval)` converts these string representations back into Python lists or arrays using `literal_eval` from the `ast` module, which safely evaluates a string containing a Python literal or container display.

- `.apply(np.array)` then converts these lists or arrays into NumPy arrays for efficient numerical operations. This is crucial for any subsequent computations or processing involving these embeddings, such as calculating distances or similarities between vectors.

### Seting up OpenAI API client

```python
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
```

- `load_dotenv()` loads environment variables from a `.env` file into the environment. This is particularly useful for managing sensitive data like API keys, as it keeps them out of the source code.
- `client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))` initialises an OpenAI client with the API key retrieved from the environment variables. This client is used to interact with OpenAI's API, allowing the application to perform operations like generating text completions or embeddings. The `os.getenv("OPENAI_API_KEY")` function retrieves the value of the environment variable `OPENAI_API_KEY`, which should be the API key for OpenAI.

### Create context for the question

This section of the code defines a function `create_context` that creates context for a given question by finding the most similar context from a DataFrame based on embeddings. The function utilises embeddings to measure similarity and select relevant text to form a context that can help generate more accurate and relevant responses to the question. 

The `create_context` function is particularly useful in applications involving question-answering systems where providing relevant context can significantly improve the quality of the answers generated by NLP models. By leveraging semantic embeddings to identify and aggregate the most relevant texts, the function ensures that the generated context is closely related to the question, thereby enhancing the model's ability to produce accurate and informed responses.

```python
def create_context(question, df, max_len=1800, size="ada"):
    """
    Create a context for a question by finding the most similar context from the dataframe
    """

    # Get the embeddings for the question
    #q_embeddings = client.embeddings.create(model="text-embedding-ada-002", input=question)['data'][0]['embedding']
    response = client.embeddings.create(input=[question], model="text-embedding-ada-002")
    q_embeddings = response.data[0].embedding

    # Calculate cosine distances and update the dataframe
    def calculate_cosine_distance(row_embedding):
        return cosine(q_embeddings, row_embedding)

    # Apply the function to each row's embeddings to calculate distances
    df['distances'] = df['embeddings'].apply(calculate_cosine_distance)

    returns = []
    cur_len = 0

    # Sort by distance and add the text to the context until the context is too long
    for i, row in df.sort_values('distances', ascending=True).iterrows():

        # Add the length of the text to the current length
        cur_len += row['n_tokens'] + 4

        # If the context is too long, break
        if cur_len > max_len:
            break

        # Else add it to the text that is being returned
        returns.append(row["text"])

    # Return the context
    return "\n\n###\n\n".join(returns)
```
Here's how it works:

**Parameters:**
- `question`: The question for which context is being created.
- `df`: A DataFrame containing text data and their embeddings.
- `max_len`: The maximum length of the context in tokens.
- `size`: The model size used for embeddings, defaulted to "ada".

**Steps in the Function**

**1. Generate Embeddings for the Question:**
- The function starts by creating embeddings for the input question using OpenAI's API, specifically calling the `client.embeddings.create` function with the model `text-embedding-ada-002`. This model generates a numerical vector (embedding) that represents the semantic content of the question.
- `response.data[0].embedding` extracts the embedding from the API's response.

**2. Calculate Cosine Distances:**
- A nested function `calculate_cosine_distance` is defined to calculate the cosine distance between the question's embedding and each row's embedding in the DataFrame. Cosine distance is a measure used to determine the similarity between two vectors, with lower values indicating higher similarity.
- This function is applied to each row's embeddings in the DataFrame (`df['embeddings'].apply(calculate_cosine_distance)`), calculating the distance between the question's embedding and the embedding of each piece of text in the DataFrame. The results are stored in a new column, `df['distances']`.

**3. Select Text for Context:**
- The DataFrame is sorted based on the calculated distances in ascending order, meaning texts with embeddings most similar to the question's embedding come first.
- The function then iterates over the sorted DataFrame, accumulating texts to form the context until reaching the `max_len` limit. This ensures the context is built from texts most relevant to the question.
- cur_len keeps track of the current length of the context in tokens, and `returns` is a list that accumulates the selected texts.
- Texts are added to the context until the `max_len` is reached, at which point the loop breaks. This mechanism ensures the context doesn't exceed the specified maximum length, making it suitable for models with input size limitations.

**4. Return the Context:**
- The selected texts are joined with a separator (`"\n\n###\n\n"`) to clearly demarcate different pieces of text within the context.
- The function returns this concatenated string as the context for the given question.

### Generate answers

The `answer_question` function is designed to generate an answer to a given question by utilizing the context created from texts within a DataFrame and leveraging OpenAI's GPT model.

```python
def answer_question(
    df,
    question,
    model="gpt-3.5-turbo",
    max_len=1800,
    size="ada",
    debug=False,
    max_tokens=150,
    stop_sequence=None
):
    """
    Answer a question based on the most similar context from the dataframe texts
    """
    context = create_context(
        question,
        df,
        max_len=max_len,
        size=size,
    )
    # If debug, print the raw model response
    if debug:
        print("Context:\n" + context)
        print("\n\n")
        print("\n\nQuestion:\n" + question)

    try:
        # Create a chat completion using the question and context
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": f"Answer this question based on the context below: {question}. If the question can't be answered based on the context, say \"SALSA\"\n\n"},
                {"role": "user", "content": f"Context: {context}\n\n---\n\nAnswer:"},
            ],
            temperature=0,
            max_tokens=max_tokens,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            stop=stop_sequence)
        
        # Print the entire response object for inspection
        if debug:
            print("\n\nFull Response Object:\n")
            print(response)  # This line prints the entire response object

        # Extracting and returning the completion text
        completion_text = response.choices[0].message.content.strip()
        return completion_text
    
    except Exception as e:
        print(e)
        return ""
```

Here's a detailed explanation of its workflow and components:

**Parameters**
- `df`: The DataFrame containing texts and their embeddings, used to find the most relevant context for the question.
- `question`: The question to be answered.
- `model`: Specifies the GPT model to use for generating answers, defaulting to "gpt-3.5-turbo".
- `max_len`: The maximum length of the context in tokens. This helps ensure the context passed to the model does not exceed the model's maximum input size.
- `size`: The size of the model used for embeddings, defaulted to "ada".
- `debug`: A boolean flag used to print additional information for debugging purposes.
- `max_tokens`: The maximum number of tokens allowed in the model's response.
- `stop_sequence`: Optional parameter defining sequences where the model should stop generating further tokens.

**Functionality Breakdown**

1. Context Creation:
- The function begins by calling `create_context`, which generates a context for the question from the DataFrame `df`. This context is assembled based on similarity to the question's embeddings, ensuring relevance.

2. Debugging Information:
- If `debug` is `True`, the function prints the generated context and the question to the console. This feature is useful for developers to inspect the context being used and to ensure it aligns well with the question.

3. Generating an Answer:
- The function then uses OpenAI's `client.chat.completions.create` to generate an answer. This method sends a request to the specified GPT model, providing it with the context and the question structured in a way that guides the model to generate an appropriate answer based on the provided context.
- The `model` parameter specifies which GPT model version to use.
- The `messages` parameter includes both the system prompt, which instructs the model on how to use the context to answer the question, and the user prompt, which combines the context with the question itself.
- The `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, and `presence_penalty` parameters control the creativity, length, and focus of the model's response.

**4. Handling the Response:**
- The model's response is captured in the `response` variable. If `debug` is `True`, the entire response object is printed, allowing for a detailed inspection of the model's output and metadata.
- The function extracts the completion text from `response.choices[0].message.content` and strips any leading or trailing whitespace. This extracted text is the model-generated answer to the question.

**5. Error Handling:**
- A `try-except` block is used to catch and print any exceptions that occur during the API call or processing, ensuring the function does not crash the application on errors. In case of an exception, the function returns an empty string as a fallback.

**Return Value**
The function returns the generated answer as a string. This answer is based on the most similar context found within the DataFrame and is tailored to the specific question asked, leveraging the power of OpenAI's GPT model to provide informative and contextually relevant responses.

This `answer_question` function encapsulates the entire process of context-based question answering, from context generation to leveraging advanced AI models for generating responses, making it a crucial component for applications requiring dynamic and intelligent text-based interactions.

### Flask routes

These code snippets define two Flask routes for a web application. Flask uses decorators to link functions to web page routes, allowing for the creation of dynamic web applications with Python. 

Together, these routes enable a web application where users can submit questions via a form (or any other frontend that makes POST requests to `/ask`) and receive answers dynamically generated by the backend. The home route provides a simple way to serve the main HTML page of the application, potentially containing the form or interface for users to interact with the question-answering functionality.

Here's a breakdown of each route:

**The `/ask` Route**

```python
@app.route('/ask', methods=['POST'])
def ask():
    content = request.json
    question = content.get('question')

    answer = answer_question(df, question=question, debug=False)
    return jsonify({'answer': answer})
```
**- Route Definition:** `@app.route('/ask', methods=['POST'])` declares a new route `/ask` that accepts `POST` requests. This is typically used for submitting data to the server in a more secure manner than `GET` requests.

**- Function ask:** When a POST request is made to `/ask`, the ask function is executed. This function is designed to handle questions submitted as JSON payloads.

**- Extract Question:** The function extracts the `question` from the JSON payload of the request. `request.json` is a Flask feature that automatically parses JSON request data, and `content.get('question')` safely retrieves the `question` value from this dictionary.

**- Generate Answer:** It then calls the `answer_question` function, passing the DataFrame `df` and the extracted `question`. The `answer_question` function generates an answer based on the context created from `df`.

**- Return Response:** The generated answer is returned to the client as a JSON response using `jsonify({'answer': answer})`. `jsonify` is a Flask utility that converts the dictionary into a JSON formatted response, setting the appropriate Content-Type header.

**The `/ (Home)` Route**

```python
@app.route('/')
def home():
    return app.send_static_file('index.html')
```
**- Route Definition:** `@app.route('/')` defines the root route of the web application, which is the default page users see when they visit the base URL.

**- Function home:** The `home` function is called when a request is made to the root route. It serves the purpose of displaying the home page of the web application.

**- Serve Static File:** The function uses `app.send_static_file('index.html')` to directly serve the index.html file from the static folder. This method is a convenient way to serve static files without requiring a separate web server or additional configuration.

### Conditional statement

This final section of the code is a conditional statement used to check if the script is being executed as the main program and not being imported as a module in another script. Here's an explanation of its components and purpose:

```python
if __name__ == '__main__':
    app.run(debug=False)
```

- `__name__ == '__main__'`: This is a special condition in Python that evaluates to `True` if the script is run directly (e.g., `python script.py` from the command line). When a Python script is executed, Python sets the `__name__` variable to '`__main__`' in the script's global namespace. However, if the script is being imported into another script as a module, `__name__ `is set to the script's/module's name, and this condition evaluates to `False`.
- `app.run(debug=False)`: This line starts the Flask application.
    - `app` is the Flask application instance created earlier in the script.
    - The `run` method is called to start the Flask integrated development server, making the application listen for incoming requests on a default port (usually `5000`) on the localhost (`127.0.0.1`).
    - The `debug` parameter is set to `False`, indicating that the application should not run in debug mode. When `debug` is `True`, Flask provides a more verbose output on the console and automatically reloads the server upon code changes, which is useful during development.

**Purpose**
This setup is common in Flask applications to allow the same script to be versatile: it can be run as a standalone server (useful during development and testing) or imported as a module in a larger application or framework (e.g., when deploying with a WSGI server like Gunicorn).

**Summary**
This section marks the entry point of the Flask application, ensuring it only runs when the script is executed directly. It's a common pattern used in Python scripts to provide flexibility in how scripts are used and deployed.