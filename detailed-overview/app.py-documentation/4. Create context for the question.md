# Create context for the question

This section of the code defines a function `create_context` that creates context for a given question by finding the most similar context from a DataFrame based on embeddings. The function utilises embeddings to measure similarity and select relevant text to form a context that can help generate more accurate and relevant responses to the question. 

The `create_context` function is particularly useful in applications involving question-answering systems where providing relevant context can significantly improve the quality of the answers generated by NLP models. By leveraging semantic embeddings to identify and aggregate the most relevant texts, the function ensures that the generated context is closely related to the question, thereby enhancing the model's ability to produce accurate and informed responses.

```python
def create_context(question, df, max_len=1800, size="ada"):
    """
    Create a context for a question by finding the most similar context from the dataframe
    """

    # Get the embeddings for the question
    #q_embeddings = client.embeddings.create(model="text-embedding-ada-002", input=question)['data'][0]['embedding']
    response = client.embeddings.create(input=[question], model="text-embedding-ada-002")
    q_embeddings = response.data[0].embedding

    # Calculate cosine distances and update the dataframe
    def calculate_cosine_distance(row_embedding):
        return cosine(q_embeddings, row_embedding)

    # Apply the function to each row's embeddings to calculate distances
    df['distances'] = df['embeddings'].apply(calculate_cosine_distance)

    returns = []
    cur_len = 0

    # Sort by distance and add the text to the context until the context is too long
    for i, row in df.sort_values('distances', ascending=True).iterrows():

        # Add the length of the text to the current length
        cur_len += row['n_tokens'] + 4

        # If the context is too long, break
        if cur_len > max_len:
            break

        # Else add it to the text that is being returned
        returns.append(row["text"])

    # Return the context
    return "\n\n###\n\n".join(returns)
```
Here's how it works:

**Parameters:**
- `question`: The question for which context is being created.
- `df`: A DataFrame containing text data and their embeddings.
- `max_len`: The maximum length of the context in tokens.
- `size`: The model size used for embeddings, defaulted to "ada".

**Steps in the Function**

**1. Generate Embeddings for the Question:**
- The function starts by creating embeddings for the input question using OpenAI's API, specifically calling the `client.embeddings.create` function with the model `text-embedding-ada-002`. This model generates a numerical vector (embedding) that represents the semantic content of the question.
- `response.data[0].embedding` extracts the embedding from the API's response.

**2. Calculate Cosine Distances:**
- A nested function `calculate_cosine_distance` is defined to calculate the cosine distance between the question's embedding and each row's embedding in the DataFrame. Cosine distance is a measure used to determine the similarity between two vectors, with lower values indicating higher similarity.
- This function is applied to each row's embeddings in the DataFrame (`df['embeddings'].apply(calculate_cosine_distance)`), calculating the distance between the question's embedding and the embedding of each piece of text in the DataFrame. The results are stored in a new column, `df['distances']`.

**3. Select Text for Context:**
- The DataFrame is sorted based on the calculated distances in ascending order, meaning texts with embeddings most similar to the question's embedding come first.
- The function then iterates over the sorted DataFrame, accumulating texts to form the context until reaching the `max_len` limit. This ensures the context is built from texts most relevant to the question.
- cur_len keeps track of the current length of the context in tokens, and `returns` is a list that accumulates the selected texts.
- Texts are added to the context until the `max_len` is reached, at which point the loop breaks. This mechanism ensures the context doesn't exceed the specified maximum length, making it suitable for models with input size limitations.

**4. Return the Context:**
- The selected texts are joined with a separator (`"\n\n###\n\n"`) to clearly demarcate different pieces of text within the context.
- The function returns this concatenated string as the context for the given question.

### [Click to continue...](/detailed-overview/app.py-documentation/5.%20Generate%20answers.md)

### [Back to overview](/detailed-overview/3.%20Detailed%20overview.md)