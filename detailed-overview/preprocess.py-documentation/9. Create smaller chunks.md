# Create smaller chunks

The `split_into_many()` function splits blocks of text into smaller chunks, with each containing no more than the specified number of tokens (`max_tokens`).

```python
# Function to split the text into chunks of a maximum number of tokens
def split_into_many(text, max_tokens = max_tokens):

    # Split the text into sentences
    sentences = text.split('. ')

    # Get the number of tokens for each sentence
    n_tokens = [len(tokenizer.encode(" " + sentence)) for sentence in sentences]

    chunks = []
    tokens_so_far = 0
    chunk = []

    # Loop through the sentences and tokens joined together in a tuple
    for sentence, token in zip(sentences, n_tokens):

        # If the number of tokens so far plus the number of tokens in the current sentence is greater
        # than the max number of tokens, then add the chunk to the list of chunks and reset
        # the chunk and tokens so far
        if tokens_so_far + token > max_tokens:
            chunks.append(". ".join(chunk) + ".")
            chunk = []
            tokens_so_far = 0

        # If the number of tokens in the current sentence is greater than the max number of
        # tokens, go to the next sentence
        if token > max_tokens:
            continue

        # Otherwise, add the sentence to the chunk and add the number of tokens to the total
        chunk.append(sentence)
        tokens_so_far += token + 1

    return chunks
```

This is particularly useful for preparing text for processing with models that have a maximum input size limit. Here's a breakdown of how it works:

**Parameters**
- `text` is the block of text to be split
- `max_tokens` is an optional paramater to specify the maximum number of tokens allowed per chunk. It defaults to a global `max_tokens` value defined elsewhere.

**Logic**
**1. Sentence Splitting:** The function starts by splitting the `text` into sentences based on the period followed by a space (`.`) delimiter. This approach assumes that sentences are a good boundary for splitting, aiming to preserve sentence integrity within each chunk.

**2. Token Count Calculation:**
- For each sentence, the function calculates the number of tokens it contains by encoding the sentence using the previously loaded `tokenizer` (from `tiktoken.get_encoding("cl100k_base")`).
- The `tokenizer.encode` method is called with a space concatenated before the sentence to ensure proper tokenization, especially at the beginning of sentences.
- The result is a list, `n_tokens`, where each element corresponds to the token count of each sentence.

**3. Chunk Assembly:**
- The function then iterates through each sentence and its associated token count, attempting to assemble chunks of text that adhere to the `max_tokens` limit.
- `chunks` is a list to store the final text chunks.
- `tokens_so_far` tracks the cumulative number of tokens in the current chunk being assembled.
- `chunk` is a temporary list to hold sentences that will form the current chunk.

**4. Finalise chunks**
- In case there's remaining text in the `chunk` after the loop (meaning the text didn't reach another `max_tokens` limit), this text is concatenated and added to `chunks` as the final chunk. This ensures completeness of the text processing, with no content left behind

**Process**
As it iterates through sentences, the function checks if adding the next sentence would exceed the max_tokens limit. If it would:
- The sentences accumulated in `chunk` so far are joined with `.` (to reconstruct the original sentence boundaries) and added as a new entry in `chunks`.
- `chunk` and `tokens_so_far` are reset to start assembling a new chunk.

If a sentence's token count alone exceeds `max_tokens`, that sentence is skipped. 

> *This is a simplification and might not be ideal for all use cases, as it could result in the omission of important text. Depending on the application, you might want to further split such sentences or handle them differently.*

This process continues until all sentences have been processed. If there's any remaining text in `chunk` after the loop, it's also joined and added to `chunks`.

The function ultimately returns `chunks`, a list of text segments where each segment is designed to be within the specified `max_tokens` limit, making them suitable for processing with token limit-constrained models or APIs.

### [Click to continue...](/detailed-overview/preprocess.py-documentation/10.%20Check%20dataframe%20does%20not%20exceed%20max_tokens.md)

### [Go back to overview](/detailed-overview/3.%20Detailed%20overview.md)