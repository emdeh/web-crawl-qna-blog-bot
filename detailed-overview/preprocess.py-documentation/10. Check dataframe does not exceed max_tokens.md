# Check DataFrame does not exceed `max_tokens`.

This segment is designed to process a DataFrame containing text data by ensuring each piece of text does not exceed a specified maximum number of tokens (`max_tokens`). It aims to create a new DataFrame where each row contains text within the token limit, using the previously defined `split_into_many` function to split longer texts into appropriately sized chunks.

```python
shortened = []

# Loop through the dataframe
for row in df.iterrows():

    # If the text is None, go to the next row
    if row[1]['text'] is None:
        continue

    # If the number of tokens is greater than the max number of tokens, split the text into chunks
    if row[1]['n_tokens'] > max_tokens:
        shortened += split_into_many(row[1]['text'])

    # Otherwise, add the text to the list of shortened texts
    else:
        shortened.append( row[1]['text'] )

df = pd.DataFrame(shortened, columns = ['text'])
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))
df.n_tokens.hist()
```
Here's a breakdown of its functionality:
**- `shortened` List:** Initialises an empty list named `shortened` to store the processed texts, which will either be the original texts (if they're within the token limit) or chunks of texts (if the original text exceeds the token limit).

The code iterates over each row in the DataFrame `df` using `df.iterrows()`, which provides an iterator yielding index and row data for each row.

**1. Skip Null Texts:** If the text column of the current row is `None` (indicating missing or null text), the loop continues to the next iteration without adding anything to `shortened`.

**2. Handling Long Texts:** If the `n_tokens` value for the row's text exceeds `max_tokens`, indicating that the text is too long based on the token limit:
- The `split_into_many` function is called with the row's text, resulting in a list of text chunks, each adhering to the token limit. These chunks are then added (`+=`) to the `shortened` list.

**3. Handling Short Texts:** If the text's token count does not exceed `max_tokens`, the original text is directly appended to the `shortened` list as it already complies with the token limit.

**Creating a New DataFrame**
After processing all rows, a new DataFrame is created from the `shortened` list, with each element (text or chunk of text) becoming a row in the new DataFrame. This ensures that all text entries in the new DataFrame are within the specified token limit.

**Recalculating Token Counts**
The new DataFrame recalculates the number of tokens for each text (or text chunk) using the tokenizer's `encode` method. This calculation is stored in a new `n_tokens` column, providing an updated token count for each row based on the potentially modified texts.

**Visualizing Token Distribution**
Finally, the code generates a histogram of the `n_tokens` column in the new DataFrame. This visualization helps understand the distribution of token counts across the processed texts, ensuring that the splitting logic has effectively regulated text lengths according to the `max_tokens` limit.

### [Click to continue...](/detailed-overview/preprocess.py-documentation/11.%20Generate%20embeddings.md)

### [Go back to overview](/detailed-overview/3.%20Detailed%20overview.md)