# Extract content from links

The `crawl(url)` function is deisgned to systematically visit the links, extract, and save the text content of each page to a file. It also follows the links to other pages within the same domain to continue the crawling process.

```python
def crawl(url):
    # Parse the URL and get the domain
    local_domain = urlparse(url).netloc

    # Create a queue to store the URLs to crawl
    queue = deque([url])

    # Create a set to store the URLs that have already been seen (no duplicates)
    seen = set([url])

    # Create a directory to store the text files
    if not os.path.exists("text/"):
            os.mkdir("text/")

    if not os.path.exists("text/"+local_domain+"/"):
            os.mkdir("text/" + local_domain + "/")

    # Create a directory to store the csv files
    if not os.path.exists("data"):
            os.mkdir("data")

    # While the queue is not empty, continue crawling
    while queue:

        # Get the next URL from the queue
        url = queue.pop()
        print(url) # for debugging and to see the progress

        # Save text from the url to a <url>.txt file
        with open('text/'+local_domain+'/'+url[8:].replace("/", "_") + ".txt", "w", encoding="UTF-8") as f:

            # Get the text from the URL using BeautifulSoup
            soup = BeautifulSoup(requests.get(url).text, "html.parser")

            # Get the text but remove the tags
            text = soup.get_text()

            # If the crawler gets to a page that requires JavaScript, it will stop the crawl
            if ("You need to enable JavaScript to run this app." in text):
                print("Unable to parse page " + url + " due to JavaScript being required")

            # Otherwise, write the text to the file in the text directory
            f.write(text)

        # Get the hyperlinks from the URL and add them to the queue
        for link in get_domain_hyperlinks(local_domain, url):
            if link not in seen:
                queue.append(link)
                seen.add(link)

crawl(full_url)
```

Here is a breakdown:

**1. Parse the Starting URL:** The function uses `urlparse(url).netloc` to extract the domain name (`local_domain`) from the starting URL. This domain name is used to limit the crawl to pages within the same domain.

**2. Initialise Data Structures:**
- A queue `queue` (implemented with `deque`) is initialised with the starting URL. The queue manages the URLs to visit in a first-in, first-out (FIFO) manner.
- A set `seen` is initialised with the starting URL to keep track of URLs that have already been visited or seen, preventing the crawler from visiting the same page multiple times.

**3. Prepare Directories:** The function checks if the directories for storing text files (`text/<domain>/`) and CSV files (`data`) exist. If not, it creates them. This ensures that there's a place to save the extracted data.

**Crawling Loop:**
- The crawler enters a loop that continues as long as there are URLs in the `queue` to be visited.
- It takes the next URL from the `queue`, prints it for debugging purposes, and proceeds to process it.

**5. Extract and Save Text Content:**
- The function fetches the HTML content of the current URL using `requests.get(url).text` and parses it with `BeautifulSoup` to create a `soup` object.
- It then extracts the textual content of the page using `soup.get_text()`, which removes all HTML tags, leaving only the text.
- The text is saved to a file named after the URL (with slashes replaced by underscores to form a valid filename) in the `text/<domain>/` directory. Before writing, it checks if the text indicates that JavaScript is required to view the page content, in which case it prints a message and skips saving.

**6. Follow Hyperlinks:**
- The function calls `get_domain_hyperlinks(local_domain, url)` to get a list of hyperlinks from the current page that are within the same domain.
- It adds these hyperlinks to the `queue` if they haven't been seen before, updating the seen set accordingly. This way, the crawler systematically explores the website, following links to discover and process all accessible pages within the domain.

**7. Recursive Crawling:** By adding new links to the `queue` and marking visited URLs in `seen`, the crawler recursively visits the entire website or until no more new links are found within the domain.

This crawler is effective for extracting text data from websites for analysis, search indexing, or archiving purposes. It respects the site's domain boundaries, ensuring that it does not stray into external sites.

### [Click to continue...](/detailed-overview/preprocess.py-documentation/5.%20Remove%20new%20lines.md)

### [Go back to overview](/detailed-overview/3.%20Detailed%20overview.md)