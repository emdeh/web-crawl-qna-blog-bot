# Fetch and filter hyperlinks.

The `get_domain_hyperlinks(local_domain, url)` function fetches the hyperlinks and filters those links so that only the ones belonging to the specified `local_domain` are returned.

```python
# Function to get the hyperlinks from a URL that are within the same domain
def get_domain_hyperlinks(local_domain, url):
    clean_links = []
    for link in set(get_hyperlinks(url)):
        clean_link = None

        # If the link is a URL, check if it is within the same domain
        if re.search(HTTP_URL_PATTERN, link):
            # Parse the URL and check if the domain is the same
            url_obj = urlparse(link)
            if url_obj.netloc == local_domain:
                clean_link = link

        # If the link is not a URL, check if it is a relative link
        else:
            if link.startswith("/"):
                link = link[1:]
            elif link.startswith("#") or link.startswith("mailto:"):
                continue
            clean_link = "https://" + local_domain + "/" + link

        if clean_link is not None:
            if clean_link.endswith("/"):
                clean_link = clean_link[:-1]
            clean_links.append(clean_link)

    # Return the list of hyperlinks that are within the same domain
    return list(set(clean_links))
```

Here is a breakdown:

**1. Fetch All Hyperlinks:** The function starts by calling `get_hyperlinks(url)`, which retrieves all hyperlinks from the given URL's webpage. This returns a list of URLs found in `<a>` tags on the page.

**2. Initialise a List for Clean Links:** A list named `clean_links` is initialised to store the filtered hyperlinks that are within the specified domain.

**3. Iterate Through the Links:** The function iterates through the set of links returned by `get_hyperlinks(url)` to ensure each link is processed only once, even if it appears multiple times on the page.

**URL Validation and Domain Check:**
- For each link, the function first checks if it is an absolute URL that includes the domain name. This is done using the `re.search(HTTP_URL_PATTERN, link)` which matches links that start with "http://" or "https://".
- If the link is an absolute URL, the function uses `urlparse(link)` to parse the URL and compare its domain (`netloc`) with the `local_domain` specified. If they match, the link is considered "clean" and belongs to the same domain.
- If the link does not match the `HTTP_URL_PATTERN`, it's considered a relative link or a special link (like those starting with `#` for page anchors or `mailto:` for email addresses). Relative links are presumed to be internal to the domain, but special links are ignored.

**5. Handling Relative Links:** For relative links, the function constructs a full URL by concatenating "https://" with the `local_domain` and the relative path. It ensures that links starting with "/" don't duplicate the slash when concatenated with the domain.

**6. Final URL Adjustments:** Before adding a URL to `clean_links`, the function checks if it ends with a slash ("/") and removes it if present. This normalisation step ensures consistency in the URLs being processed and stored.

**7. Return Unique Clean Links:** Finally, the function returns a list of unique clean links that are within the same domain. This is achieved by converting `clean_links` to a set and then back to a list, which removes any duplicates.

This function is particularly useful in web crawling tasks where the goal is to systematically explore and collect data from a website by following links that keep the crawler within the same domain, thereby avoiding external sites.

### [Click to continue...](/detailed-overview/preprocess.py-documentation/4.%20Extract%20content%20from%20links.md)

### [Go back to overview](/detailed-overview/3.%20Detailed%20overview.md)