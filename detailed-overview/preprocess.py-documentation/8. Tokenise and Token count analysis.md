# Tokenise and Token Count Analysis

This section tokenises the data to prepare it for embedding generation and other NLP tasks.

```python

import tiktoken
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load the cl100k_base tokenizer which is designed to work with the ada-002 model
tokenizer = tiktoken.get_encoding("cl100k_base")

df = pd.read_csv('data/scraped.csv', index_col=0)
df.columns = ['title', 'text']

# Tokenize the text and save the number of tokens to a new column
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))

# Visualize the distribution of the number of tokens per row using a histogram
df.n_tokens.hist()

max_tokens = 500
```

`tiktoken.get_encoding("cl100k_base")` is used to load a tokenizer that is compatible with the `ada-002` model. This tokenizer is applied to the text data to tokenize it.

 For each row in the DataFrame, the number of tokens (after tokenization) is calculated and stored in a new column (`n_tokens`). This allows for analysing the length of documents in terms of tokens, which is relevant for understanding the dataset's distribution and for optimizing model input sizes.

Histogram Visualization: A histogram of the `n_tokens` column is generated to visualize the distribution of token counts across the dataset. This can help identify common document lengths and any outliers, informing decisions about data preprocessing (e.g., splitting long documents) and model configuration (e.g., setting appropriate maximum token limits).

The `max_tokens` variable is set to 500, indicating a threshold or parameter that could be used in subsequent processing steps, such as when generating embeddings or splitting documents into smaller chunks to ensure they fit within the input size limits of certain models.

This section of the code is crucial for preparing scraped web content for NLP tasks, ensuring data cleanliness, and structuring the data in a way that facilitates easy analysis and processing with machine learning models.

### [Click to continue...](/detailed-overview/preprocess.py-documentation/9.%20Create%20smaller%20chunks.md)

### [Go back to overview](/detailed-overview/3.%20Detailed%20overview.md)